{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe68f7e6-5fb8-478d-9ec4-5b75942f299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd82b206-b7a2-40c1-8905-c0b95f2cd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGSMITH_PROJECT']=\"ai-agents-rags\"\n",
    "os.environ['LANGSMITH_TRACING']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a80b0ee-d66c-4cf0-90dd-513a1eb3ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_TRACING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6042c8-04dd-44d0-83fb-4ee963bb5042",
   "metadata": {},
   "source": [
    "### Find User Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1678fe4-0536-42f6-9fd5-306b3aea2918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user-agent': 'python-requests/2.32.3'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://httpbin.org/user-agent')\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34548499-6c4f-4f77-a250-15c8f56ad5e5",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1886521d-4928-4bd5-af78-9fd7cefc07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4323a0-bed1-4660-a699-699aad31906b",
   "metadata": {},
   "source": [
    "## Loading, Splitting, Retrieval and Generation: from a webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f1cce7-5d37-4a7f-8f74-2691cfb21caf",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "https://python.langchain.com/docs/integrations/document_loaders/\n",
    "![Indexing](./indexing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a6d433-8e9e-4a47-850b-cfcd90fe2a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d637b07-afc4-4a0c-9e59-7522414873af",
   "metadata": {},
   "source": [
    "### Splitting\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15698c6d-b75f-403b-83a2-2fcc048430be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed https://python.langchain.com/docs/integrations/vectorstores/\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e3516-ea35-4753-bb24-1fbd598f95ac",
   "metadata": {},
   "source": [
    "### Generation\n",
    "![Generation](./generation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dcff645-4829-413d-bbf2-471337fc3dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to facilitate planning and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts enhance this process by encouraging step-by-step reasoning and exploring multiple possibilities. It can be achieved through simple prompting, task-specific instructions, or human inputs, and may also involve external classical planners for long-horizon planning.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2b0f03-134e-4b77-8450-debc3e653044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To optimize LLM calls in a RAG setup during conversations, consider implementing a long-term memory system using external vector stores for efficient retrieval of historical context without overloading the LLM's short-term memory. Additionally, utilize summarization techniques to condense past interactions, allowing the model to maintain relevant information while minimizing context length. This approach helps balance the need for historical awareness with the limitations of the LLM's context capacity.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"\"\"\n",
    "How to optimize the LLM calls in a RAG setup, for example during a conversation - What measures to be taken to remember the past without stacking the converstion?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ccdb75b-ba69-4017-8bfc-63b3c6eda55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The interaction between the LLM and the vector store occurs through a retrieval model that surfaces relevant context based on recency, importance, and relevance to the current query. The LLM uses this retrieved information to inform its responses and generate outputs, while also leveraging both short-term and long-term memory for improved performance. This process allows the LLM to access a larger knowledge pool and adapt its behavior based on past experiences.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"\"\"\n",
    "How exactly the LLM - Vector store interaction happens during the conversation?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff29e27-c073-464e-b95b-af88c5ecfefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the scenario described, if the chat API loses context from the first five interactions, it typically relies on its short-term memory, which is limited. However, if the 8th interaction needs to refer back to older data, it may utilize an external vector store to retrieve relevant information. This allows the model to access a larger knowledge pool beyond its immediate context.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"\"\"\n",
    "Take the case openai chat completion api, we call the api with a specific question. \n",
    "Let us say during the 8th interaction of the conversation - context lost the first 5 interactions.\n",
    "But the 8th interaction should refer the older data. What exactly going on here, Is the chat api makes a call to the vector store?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2287c6f-d6cd-4193-891d-c7c5dffd80b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The vector store is significant because it provides access to a larger knowledge pool, allowing the LLM to retrieve information beyond its limited context capacity. This external memory capability enables the model to retain and recall information over extended periods, enhancing its performance in tasks requiring long-term memory. Without access to the vector store, the LLM's ability to utilize historical information and detailed instructions is severely restricted.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"\"\"\n",
    "Assume the llm has no access to the vector store, then whats the significance of the vector store?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441d3a7e-e7d5-4e1e-af24-d4936c0eb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6dc69b3-05c4-4171-9c66-95b96ac66e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ceb26d-0e8c-40dd-b53b-a07b3f956ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#https://python.langchain.com/docs/integrations/text_embedding/openai/\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97714d17-2704-466a-94ce-b6c3717929a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8806915835035412\n"
     ]
    }
   ],
   "source": [
    "# https://platform.openai.com/docs/guides/embeddings#faq\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
